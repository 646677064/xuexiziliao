# 加速

## 1，剪枝

## 陈丹琦团队新作：5%成本拿下SOTA，“羊驼剪毛”大法火了

只用**3%的计算量**、**5%的成本**取得SOTA，统治了1B-3B规模的开源大模型。

这一成果来自普林斯顿**陈丹琦**团队，名为**LLM-Shearing**大模型剪枝法。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmebvIAMSB88KpkRg5ZCklXGtLUlahoFbtfNQqjzSd2PtJInrdvOmLlA/640?wx_fmt=png)

以羊驼LLaMA 2 7B为基础，通过**定向结构化剪枝**得到1.3B和3B剪枝后的Sheared-LLama模型。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmjiaXfW3ChsibWyg2WZP1ibOoybJmYHTY712aVnENqhicPqwCwL1R2uToVg/640?wx_fmt=gif)

分别在下游任务评估上超越之前的同等规模模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmibpq8iaw4tNqKdwS6H9EOA3P3bawCQ296icgSEFzZicw1DTLdd0wKZGhtQ/640?wx_fmt=png)

一作夏梦舟表示，“比从头开始预训练划算很多”。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmqscM8bGXZ0U3LQdZn1A0aLAWzicesWTJKv6UJiax5Aq1sUlibvWaT2DBQ/640?wx_fmt=png)

论文中也给出了剪枝后的Sheared-LLaMA输出示例，表示尽管规模只有1.3B和2.7B，也已经能生成连贯且内容丰富的回复。

相同的“扮演一个半导体行业分析师”任务，2.7B版本的回答结构上还要更清晰一些。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKm91iayu5yqlDibAxib7HwTAy7icgO7lqKAsKzicIDBodORgQINDR8Hon8N2g/640?wx_fmt=png)

团队表示虽然目前只用Llama 2 7B版做了剪枝实验，但该方法**可以扩展到其他模型架构**，也能**扩展到任意规模**。

另外还有一个好处，剪枝后可自行选用优质的数据集继续预训练。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmlNxLiad018yObibsROOzic7bCyaahC5QE1rw0zV5Ks83ZcClVg8rH1fjw/640?wx_fmt=png)

有开发者表示，6个月前还几乎所有人都认为65B以下的模型没有任何实际用处。

> 照这样下去，我敢打赌1B-3B模型也能产生巨大价值，如果不是现在，也是不久以后。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKm8QNWF0LII5WrnDnorvzibHrAfGs0FtmepOKeicqEqibJdqVLgP8zkgJRg/640?wx_fmt=png)

**把剪枝当做约束优化**

LLM-Shearing，具体来说是一种**定向结构化剪枝**，将一个大模型剪枝到指定的目标结构。

之前的剪枝方法可能会导致模型性能下降，因为会删除一些结构，影响表达能力。

新方法将剪枝看成一种约束优化问题，学习剪枝掩码矩阵来搜索与指定结构匹配的子网络，同时以最大化性能为目标。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmFhViatJjst49SD2KXAEtYKTVWoXIbf6SHRMEJxJEaaYyd4iabJqgICwA/640?wx_fmt=png)

接下来对剪枝过的模型进行继续预训练，在一定程度上恢复剪枝造成的性能损失。

在这个阶段，团队发现剪枝过的模型与从头训练的模型对不同数据集的损失下降速率不一样，产生数据使用效率低下的问题。

为此团队提出了**动态批量加载**（Dynamic Batch Loading），根据模型在不同域数据上的损失下降速率动态调整每个域的数据所占比例，提高数据使用效率。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKm7KXIH86KNPSEibxbAASZcPCvuiaz5Bf0fmZBhNUgOiaSNGFYUpuXsYib5g/640?wx_fmt=png)

实验发现，虽然剪枝模型与从头训练的同等规模模型相比，虽然一开始表现差得多，但继续预训练可以迅速提高，最终超越。

这表明从强大的基础模型中剪枝，可以为继续预训练提供更好的初始化条件。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmbNfQV6ObjgL8D4bugV1NjcHGZejykUjDHQPpBia9seHlfbWKibzCKtBw/640?wx_fmt=png)

**将持续更新，来一个剪一个**

论文作者分别为普林斯顿博士生**夏梦舟**、**高天宇**，清华**Zhiyuan Zeng**，普林斯顿助理教授**陈丹琦**。

夏梦舟，本科毕业于复旦，硕士毕业于CMU。

高天宇，本科毕业于清华，是2019年清华特奖得主。

两人都是陈丹琦的学生，陈丹琦现在为普林斯顿助理教授，普林斯顿 NLP小组的共同领导者。

最近在个人主页中，陈丹琦更新了她的研究方向。

“这些日子主要被开发大模型吸引”，正在研究的主题包括：

- 检索如何在下一代模型中发挥重要作用，提高真实性、适应性、可解释性和可信度。

- 大模型的低成本训练和部署，改进训练方法、数据管理、模型压缩和下游任务适应优化。

- 还对真正增进对当前大模型功能和局限性理解的工作感兴趣，无论在经验上还是理论上。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmMu825EIMnKVbgQptkuz2qJrxnNaDbWORUzhib5MZLzEMAWb8PvMxgYQ/640?wx_fmt=png)

目前Sheared-Llama已在Hugging Face上提供。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmHvJXOb0we2eE9fTEd1icAxG3CWaQfA8WiacoGibvRz5bp1anISIyjaibzw/640?wx_fmt=png)

团队表示，开源库还会保持更新。

更多大模型发布时，来一个剪一个，持续发布高性能的小模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmBjPz0w4vmjFtyHDdBdqxTeZ5cX0WIN3GLjRXMQvfiaLib60coUGGamRQ/640?wx_fmt=png)

## One More Thing

不得不说，现在大模型实在是太卷了。

一作Mengzhou Xia刚刚发布一条更正，表示写论文时还是SOTA，论文写好就已经被最新的Stable-LM-3B超越了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDs1syqaaic8Myh6pW0GWXKmibrcTux6xO5uuWvdGk8iafkvhEFibfFnY5SpwGmyiajnQAGbroEp8QW9uA/640?wx_fmt=png)

论文地址：  
https://arxiv.org/abs/2310.06694

Hugging Face：  
https://huggingface.co/princeton-nlp

项目主页：  
https://xiamengzhou.github.io/sheared-llama/

## 2，蒸馏

https://blog.csdn.net/HUSTHY/article/details/115174978

**模型蒸馏原理和bert模型蒸馏以及theseus压缩实战**

目录

一、模型蒸馏简介和步骤

二、模型蒸馏实战

1、Bilstm和Roberta文本分类效果展示

2、roberta蒸馏到bilstm

三、Roberta压缩——theseus理解和实战

1、bert-of-theseus思想和方法

2、利用bert-of-theseus实现的roberta压缩

模型压缩有剪枝、蒸馏和量化等一些方法，模型蒸馏实现起来比较容易简单，这里对模型蒸馏进行分析和实战效果展示。
一、模型蒸馏简介和步骤

           模型蒸馏的思想就是利用一个已经训练好的、大型的、效果比较好的Teacher模型，去指导一个轻量型、参数少的student模型去训练——在减小模型的大小和计算资源的同时，尽量把Student模型的准确率保证在Teacher模型附近。这种思想和方法在Hinton等论文Distilling the Knowledge in a Neural Network中做了详细的介绍和说明。

模型蒸馏训练的框架结构图![](https://img-blog.csdnimg.cn/20210403153239447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hVU1RIWQ==,size_16,color_FFFFFF,t_70)

第一步：训练big模型(Teacher model)，这里用到的就是正常的label(hard label)——尽量把模型的准确率训练提升上来。

第二步：联合小模型和大模型进行蒸馏训练(参考上图)。加载大模型的权重后冻结大模型的权重，得到输出soft target；小模型对soft target和hard target(数据的真实label)进行损失计算，对损失进行加权求和，然后在更新梯度，从而更新小模型的参数。值得注意的是大模型和小模型的输出计算loss的时候，需要对输出进行一个整除T的操作——论文提出的softmax-T：

这里的T是为了使得logit输出的各个类别的概率比较平滑，使得分布比较均匀，小模型在训练的时候就能学习到概率比较小的类别的一些信息。

总体loss如下：

一般而言这里的loss加权选择1:1就可以了，具体设计到loss函数的选择L_hard就选择交叉熵损失函数，L_soft可以选择相对熵KLDivLoss函数、MseLoss函数、CosineEmbeddingLoss(有人用过，我没使用过)。

具体选择什么样的loss函数，就需要针对不同的业务场景和数据来进行实验，那个效果好选择那一个(这个没有很强的理论来分析那个loss更好，一切看效果)
二、模型蒸馏实战

之前做过一个文本分类的任务，这里想进行一个文本分类任务的模型蒸馏实验，看看具体的效果怎么样。

首先看看大模型和小模型单独的效果如何，这里就把单独训练的过程和结果展示一下，具体的代码不做演示(可参考我的博客文章——基于机器学习算法和pytorch实现的深度学习模型的中文长文本多分类任务实战——TextBert和TextRNN部分内容)。特此说明该次训练采用的预训练模型是roberta模型而不是Chinese-BERT-wwm模型。
1、Bilstm和Roberta文本分类效果展示

Bilstm——TextRNN训练过程和最终结果如下：

可以看到最终在验证集上的准确率是：74.65%

Roberta——TextBert训练过程和效果如下：

可以发现roberta在验证集上最好的准确率是84.72%
2、roberta蒸馏到bilstm

蒸馏示意图如下

蒸馏的思路比较简单，把上述微调训练好的roberta模型作为teacher模型，Bilstm作为student模型。然后使用不同的loss函数，进行文本分类任务的训练。核心代码如下：

        train_data = ReadDataSet('train.tsv',args)
        train_loader = DataLoader(dataset=train_data, batch_size=args.batch_size, shuffle=True)
    
        dev_data = ReadDataSet('dev.tsv',args)
        dev_loader = DataLoader(dataset=dev_data, batch_size=args.batch_size, shuffle=True)
    
        teacher_model = torch.load('savedmodel/TextBert_model.bin')
        student_model = TextRNN()
    
        train(teacher_model,student_model,train_loader,dev_loader,args)

以上训练集和验证集数据加载、教师模型和学生模型定义

完整的训练代码如下：

    def train(teacher_model,student_model,train_loader,dev_loader,args):
        teacher_model.to('cuda')
        student_model.to('cuda')
    
        #teacher网络参数不更新
        for name,params in teacher_model.named_parameters():
            params.requires_grad = False
    
        # 初始学习率，student网络参数梯度更新
        optimizer_params = {'lr': 1e-3, 'eps': 1e-8}
        optimizer = AdamW(student_model.parameters(), **optimizer_params)
        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, min_lr=1e-6, patience=2, verbose=True,
                                        eps=1e-8)  # mode max表示当监控量停止上升时，学习率将减小；min表示当监控量停止下降时，学习率将减小；这里监控的是dev_acc因此应该用max
        # #teacher网络输出和student网络输出进行损失计算
        # soft_criterion = nn.KLDivLoss()
    
        #teacher网络输出和student网络输出进行损失计算
        soft_criterion = nn.MSELoss()
    
        #student网络和label进行损失计算
        hard_criterion = nn.CrossEntropyLoss()
    
        #alpha(0,1)之间——两个loss的权重系数
        alpha = args.alpha
    
        #T_softmax()的超参[1,10,20]等等值可以多测试几个
        T = 10
    
        early_stop_step = 50000
        last_improve = 0 #记录上次提升的step
        flag = False  # 记录是否很久没有效果提升
        dev_best_acc = 0
        dev_loss = float(50)
        dev_acc = 0
        correct = 0
        total = 0
        global_step = 0
        epochs = args.epochs
    
    
        for epoch in range(args.epochs):
            for step,batch in enumerate(tqdm(train_loader,desc='Train iteration:')):
                global_step += 1
                optimizer.zero_grad()
                batch = tuple(t.to('cuda') for t in batch)
                input_ids = batch[0]
                input_mask = batch[1]
                label = batch[2]
    
                student_model.train()
                stu_output = student_model(input_ids)
                tea_output = teacher_model(input_ids,input_mask).detach()
    
                #soft_loss————studetn和teach之间做loss，使用的是散度loss
                soft_loss = soft_criterion(F.log_softmax(stu_output/T,dim=1),F.softmax(tea_output/T,dim=1))*T*T
    
                # #soft_loss————studetn和teach之间做loss，使用的是logits的Mse损失
                # soft_loss = soft_criterion(stu_output,tea_output)
    
                #hard_loss————studetn和label之间的loss，交叉熵
                hard_loss = hard_criterion(stu_output,label)
    
                loss = soft_loss*alpha + hard_loss*(1-alpha)
    
                loss.backward()
                optimizer.step()
                total += label.size(0)
                _,predict = torch.max(stu_output,1)
                correct += (predict==label).sum().item()
                train_acc = correct / total
                if (step+1)%1000 == 0:
                    print('Train Epoch[{}/{}],step[{}/{}],tra_acc{:.6f} %,loss:{:.6f}'.format(epoch,epochs,step,len(train_loader),train_acc*100,loss.item()))
                if (step+1)%(len(train_loader)/2)==0:
                    dev_acc,dev_loss = dev(student_model, dev_loader)
                    dev_loss = dev_loss.item()
                    if dev_best_acc < dev_acc:
                        dev_best_acc = dev_acc
                        path = 'savedmodel/TextRnn_distillation_model_mse.bin'
                        torch.save(student_model,path)
                        last_improve = global_step
                    print("DEV Epoch[{}/{}],step[{}/{}],tra_acc{:.6f} %,dev_acc{:.6f} %,best_dev_acc{:.6f} %,train_loss:{:.6f},dev_loss:{:.6f}".format(epoch, epochs, step, len(train_loader), train_acc * 100, dev_acc * 100,dev_best_acc*100,loss.item(),dev_loss))
                if global_step-last_improve >= early_stop_step:
                    print("No optimization for a long time, auto-stopping...")
                    flag = True
                    break
                writer.add_scalar('textBert_distillation_bilstm/train_loss', loss.item(), global_step=global_step)
                writer.add_scalar('textBert_distillation_bilstm/dev_loss', dev_loss, global_step=global_step)
                writer.add_scalar('textBert_distillation_bilstm/train_acc', train_acc, global_step=global_step)
                writer.add_scalar('textBert_distillation_bilstm/dev_acc', dev_acc, global_step=global_step)
            scheduler.step(dev_best_acc)
            if flag:
                break
        writer.close()

注释写的比较详细，最重要的地方：

        #teacher网络参数不更新
        for name,params in teacher_model.named_parameters():
            params.requires_grad = False 
    ......           
    
                student_model.train()
                stu_output = student_model(input_ids)
                tea_output = teacher_model(input_ids,input_mask).detach()
    
                #soft_loss————studetn和teach之间做loss，使用的是散度loss
                soft_loss = soft_criterion(F.log_softmax(stu_output/T,dim=1),F.softmax(tea_output/T,dim=1))*T*T
    
    
                hard_loss = hard_criterion(stu_output,label)
    
                loss = soft_loss*alpha + hard_loss*(1-alpha)

注意到teacher模型参数不更新，在计算softloss的时候，对于teacher和student模型的输出需要做softmax_T的操作，然后使用KLDivLoss或者MseLoss来计算loss。

KLDivLoss结果如下

roberta 蒸馏到Bilst——KLDivLoss准确率：78.78%

MseLoss结果如下

roberta蒸馏到Bilstm采用MseLoss的准确率是80.99%

注意以上蒸馏过程中采用不同的loss函数的时候，其他的参数没有变化。

可以得出结论，蒸馏确实能提高小模型的性能，不同的loss函数也是具有不同的效果；另外还有其他的超参就没有去验证做实验了，读者可以自行去做实验。
三、Roberta压缩——theseus理解和实战

     针对Bert系列模型的蒸馏方法，有distillbert和tinybert，这些模型都是直接作用在bert预训练的阶段，然后把训练好的模型应用到下游任务，这样的压缩蒸馏方法对一般人来说不太友好。论文：ERT-of-Theseus: Compressing BERT by Progressive Module Replacing——提出了一种适合在funetune阶段对bert模型进行压缩蒸馏的方法，可以把Bert按照module replacing的方式来做压缩。

1、bert-of-theseus思想和方法

把训练分为两个过程，第一阶段使用模块替换(就是把原来的模型中的一些模块按照某些规则替换成更细更小的子层)。论文中把原始模型称为P_model,压缩后的模型称之为S_model，该训练阶段中考虑了P_model和S_model，它们都参与了训练；第二阶段单独S_model的微调阶段，就是为了让所有的S_model的模块参数训练任务中去。

第一阶段：压缩训练阶段——模块替换

它的思想——就是在训练的时候把S_model中的某一个模块按照一定的规则平行替换掉P_model对应的模块。当然这里不会在每个训练的step的时候把所有的P_model模块替换掉，不然就是直接用S_model来进行训练了。

论文提出替换的规则是：通过一个伯努利分布，采样一个随机变量，概率是p，那么P_model每个模块有p的概率替换掉，1-p的概率不被替换。

这里还有一个值得注意的地方，P_model和S_model在训练的时候，属于P_model的权重参数都要冻结起来，不参数梯度计算和更新，只有S_model的权重参数参与梯度计算和更新。

第二阶段：S_model finetune后的finetune——psot training

在第一阶段训练完成后，得到了S_model模型结构和权重，只需要把它组合成一个单独的模型，正常的进行同样的数据集和任务进行微调，起到一个精炼的作用，进一步提升S_model的效果。
2、利用bert-of-theseus实现的roberta压缩

参考bert-of-theseus的pytorch版本源码，实现了个人的MY_BERT_THESEUS项目。

模型代码：

    from bert_theseus.modeling_bert_of_theseus import BertModel
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    class TextBert(nn.Module):
        def __init__(self,args=None):
            super(TextBert,self).__init__()
            self.bert = BertModel.from_pretrained(args.model_path)
            self.dropout = nn.Dropout(0.5)
            self.cl1 = nn.Linear(768,768)
            self.cl2 = nn.Linear(768,384)
            self.cl3 = nn.Linear(384, 8)
    
        def forward(self,input_ids,attention_mask):
            embedding = self.bert(input_ids,attention_mask)[0]
            mean_embedding = torch.mean(embedding,dim=1)
            x = self.dropout(mean_embedding)
            x = F.relu(self.cl1(x))
            x = F.relu(self.cl2(x))
            logit = self.cl3(x)
            return logit

同样的这里加载Bert系列模型和抱抱脸的transformer是差不多的

from bert_theseus.modeling_bert_of_theseus import BertModel

self.bert = BertModel.from_pretrained(args.model_path)

这里的BertModel实现如下：

    class BertModel(BertPreTrainedModel):
        def __init__(self, config):
            super(BertModel, self).__init__(config)
            self.config = config
    
            self.embeddings = BertEmbeddings(config)
            self.encoder = BertEncoder(config)
            self.pooler = BertPooler(config)
    
            self.init_weights()
         def forward():
            ......

核心的在BertEncoder的实现，如下(其实就是bert-of-theseus库的实现)

    class BertEncoder(nn.Module):
        def __init__(self, config):
            super(BertEncoder, self).__init__()
            self.prd_n_layer = config.num_hidden_layers
            self.scc_n_layer = config.scc_n_layer
            assert self.prd_n_layer % self.scc_n_layer == 0
            self.compress_ratio = self.prd_n_layer // self.scc_n_layer
            self.bernoulli = None
            self.output_attentions = config.output_attentions
            self.output_hidden_states = config.output_hidden_states
            self.layer = nn.ModuleList([BertLayer(config) for _ in range(self.prd_n_layer)])
            self.scc_layer = nn.ModuleList([BertLayer(config) for _ in range(self.scc_n_layer)])
    
        def set_replacing_rate(self, replacing_rate):
            if not 0 < replacing_rate <= 1:
                raise Exception('Replace rate must be in the range (0, 1]!')
            self.bernoulli = Bernoulli(torch.tensor([replacing_rate]))
    
        def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None,
                    encoder_attention_mask=None):
            all_hidden_states = ()
            all_attentions = ()
            if self.training:
                inference_layers = []
                for i in range(self.scc_n_layer):
                    if self.bernoulli.sample() == 1:  # REPLACE
                        inference_layers.append(self.scc_layer[i])
                    else:  # KEEP the original
                        for offset in range(self.compress_ratio):
                            inference_layers.append(self.layer[i * self.compress_ratio + offset])
    
            else:  # inference with compressed model
                inference_layers = self.scc_layer
            ......

这里就仅仅是对self.scc_n_layer的定义修改为config文件来配置的。

第一步：进行roberta中模块替换训练，把模型由12层压缩为6层。

               使用bert_theseus定义的模型结构代码来初始化一个bert_theseus系列的模型，然后把微调好的roberta权重加载到该模型中。该模型有12层P_model子层——self.layer，它和roberta权重一一对应；然后还有6层S_model的子层，加载的时候torch会随机初始化，我们这边直接把roberta前6层模型权重赋值给S_model子层——self.scc_layer。代码如下：
    
        #加载并初始化大小模型
        model = TextBert(args)
        model_state_dic = model.state_dict()
    
        stand_model = torch.load('savedmodel/TextBert_model.bin')
        stand_model_state_dic = stand_model.state_dict()
    
        #把训练好的大模型权重赋值给P_model
        for k, v in model_state_dic.items():
            for name, param in stand_model_state_dic.items():
                if name==k:
                    model_state_dic[k] = param
    
        model.load_state_dict(model_state_dic)
        #给S_model赋予大模型的权重值————初始化
        scc_n_layer = model.bert.encoder.scc_n_layer
        model.bert.encoder.scc_layer = nn.ModuleList([deepcopy(model.bert.encoder.layer[index]) for index in range(scc_n_layer)])

剩下的训练和就普通的模型训练一样了，代码不展示，上结果图和训练过程的一些曲线图

训练过程收敛的比较快速，验证集在S_model的推理下准确率最高是84.16%

第二步：S_model组合成一个新的模型持续训练精调——post training

在第一步训练过程中，保存了bert_theseus的P_model的self.layer12层和S_model的self.scc_layer的6层权重以及做分类用的全连接权重。那么就需要对S_model的self.scc_layer的6层权重拿出来组合为一个新的小模型，继续在相同的数据集上训练。代码如下：

     # 初始化Bert模型为6层小模型TextBert中的self.bert是直接由transformer来实现的而不是bert_theseus
        config = BertConfig.from_pretrained(args.model_path,num_hidden_layers=6)
        model = TextBert(args,config)
        model_state_dic = model.state_dict()
    
        #加载压缩后的6层模型权重
        theseus_model_state_dic = torch.load('savedmodel/TexrBert_distillation_theseus_state_dict_scc_n_layer_6.bin')
    
    
        #把训练好的模型权重赋值给重新融合的小模型参数字典中，进行post_training
        for k,v in model_state_dic.items():
            for name,params in theseus_model_state_dic.items():
                if '.layer.' not in name:
                    if k==name or k==name.replace('.scc_layer.','.layer.'):
                        model_state_dic[k] = params
    
        #一定要重新装载，不然不生效
        model.load_state_dict(model_state_dic)

效果如下：

可以看到进行post training后准确率得到了进一步的提升：84.70%

总结

本博客介绍了经典的模型蒸馏的思想和步骤，并对文本分类任务由Roberta蒸馏到BiLstm的效果做了比对实验；同时也介绍了一种比较方便在微调阶段对Bert模型进行压缩的方法——bert-of-theseus，实验得出该方法在模型的准确率保留上效果明显。

BiLstm：74.68%

roberta：84.72%

Roberta蒸馏到BiLstm+KLDivLoss：78.78%——相比Bilstm上升4个点，相对roberta下降了6个点

Roberta蒸馏到BiLstm+MseLoss：80.99%——相比Bilstm上升6个点，相对roberta下降了4个点

Bert_of_theseus+第一阶段：84.16%，下降了0.56%

bert_of_theseus+post_training:84.70% 下降了0.02%

post_training也是有作用的。

参考文章

模型压缩实践系列之——bert-of-theseus，一个非常亲民的bert压缩方法

## 3，LoRa

#### 华为提出QA-LoRA：让微调大型语言模型‘轻装上阵’

些年，大型语言模型（LLMs）表现出了强大的语言理解能力。但是，想要将这些模型应用到不同的场景，并部署到边缘设备（例如手机）上，我们还面临一个重要的问题：这些模型通常参数众多，计算负担重，如何在不损失性能的情况下让它们变得“轻量”并适应不同需求呢？

今天为大家介绍一篇来自华为的最新研究——**QA-LoRA**技术，为微调LLMs带来了一个轻量又高效的新选择。

![](https://mmbiz.qpic.cn/mmbiz_png/gKaxjIx6bahpHneMTC9FbTxank4YqISwFj20xv1icWjuNmvribaAgKI6FVNmtViazBgAicEEvAcvXQXvANnhZJJibYA/640?wx_fmt=png)

**Paper:**  QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models  
**Link:**  https://arxiv.org/abs/2309.14717

背景

LLMs在自然语言处理领域取得了令人瞩目的成果，成为了多种任务的先进技术。

但是，它们在实际应用中的部署受到了其在**推理期间的高计算**和**内存需求**的限制。

目前，有两种主流的解决方法解决这些问题：

1. **参数高效的微调（PEFT）**：这种方法在微调模型的时，只调整其中一小部分参数，而大部分预训练的参数则保持不变。其中，低秩适应(LoRA)是最受欢迎的方法，它的主要思想是将适应器权重分解为两个低秩矩阵的乘积。尽管这样可以得到不错的性能，但模型的内存占用依然很大。

2. **参数量化**：量化旨在减少LLMs的参数或激活的位宽，从而提高其效率和可伸缩性。简而言之，就是将模型的权重参数从浮点数转化为整数，从而使模型更小更快。但当我们压缩得过猛，比如使用非常低的位数来表示时，模型的准确率会大打折扣。此外，还有一个主要的挑战是如何处理参数分布中的异常值，因为它们在量化时可能导致重大错误。

因此，很多研究者开始考虑如何将上述两种方法结合起来，即既微调又量化。一个简单的尝试是先进行PEFT微调，然后再量化，但这样做得到的模型准确性很差。而其他的尝试，要么在微调时计算太复杂，要么在微调后无法保持模型的量化状态。

最近，来自华为的研究人员提出一种新方法：量化感知低秩适应(Quantization-aware Low-rank Adaptation, QA-LoRA)。这个方法的核心思想是，为每一组预训练的权重参数引入一个调节因子，既可以增加量化的自由度，也可以减少微调的自由度。这样做有两个好处：首先，在微调过程中，模型的权重可以被量化，使得微调更加高效；其次，微调完成后，得到的模型更小，而且无需进行后续的量化处理。

实验结果表明，它不仅在各种语言理解任务上表现得很好，而且相比于其他方法，它的效果更加稳定且高效。与先前的适应方法（如LoRA和QLoRA）相比，QA-LoRA在微调和推断阶段都具有计算效率。更重要的是，由于不需要进行后训练量化，所以它不会导致准确性损失。因此，QA-LoRA为我们提供了一个简单有效的解决方案，帮助我们同时实现模型的量化和微调。

![](https://mmbiz.qpic.cn/mmbiz_png/gKaxjIx6bahpHneMTC9FbTxank4YqISwlTJNN8nNPASdU42b97cYuiaDeQrnqA73NibETYvDZXaSTX2hRFFcB63w/640?wx_fmt=png)

## 4，flash Attention

## 5，分布式训练

## 6，优化

# Transformer一作来卷多模态！学术图表也能看懂，100毫秒极速响应｜免费试玩

关注前沿科技  量子位  *2023-10-19 13:04*  *发表于北京*

这不，Transformer一作携团队也带来了新作，一个规模为80亿参数的**多模态大模型Fuyu-8B**。

而且**发布即开源**，模型权重在Hugging Face上可以看到。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCGKE9ttWXazJLa3ZhszFwJdyMzX2iarsibkakgfjJYEB8AkPibTsJu4EyXYb86cbFEWLV7SVAqcTcWA/640?wx_fmt=png)

该模型具备强大的图像理解能力。

**照片、图表、PDF、界面UI都不在话下。**

能从这么一张复杂的食物网里理清楚各个生物之间的关系。

> 提问：道格拉斯冷杉针叶缺失了，哪种生物会灭绝？
> 
> 回答：红树田鼠。

# 迎战GPT-4V！谷歌PaLI-3视觉语言模型问世，更小、更快、更强

近日，**Google Research、Google DeepMind 和 Google Cloud 共同推出了一个更小、更快、更强大的视觉语言模型（VLM）——PaLI-3，**该模型与相似的体积大 10 倍的模型相比具有显著竞争力。

研究人员使用分类目标预训练的视觉变换器（ViT）模型与对比性预训练的模型（SigLIP）进行了比较，结果发现，PaLI-3 虽然在标准图像分类基准上略微表现不佳，但基于 SigLIP 的 PaLI 在各种多模态基准测试中表现出卓越的性能，特别是在定位和文本理解方面。

相关研究论文以“*PaLI-3 Vision Language Models: Smaller, Faster, Stronger*”为题，已发表到预印本网站 arXiv 上。

![](https://mmbiz.qpic.cn/mmbiz_png/5qv5QsBmI9AhXfRoD2SngubkAnQXweIOFWn8DxB7811H2GAeMIia9rdyWDsbbQkDJXsdCDOXQZEO180E7XSbvbw/640?wx_fmt=png)

研究团队认为，仅有 50 亿参数的 PaLI-3 重新点燃了关于复杂 VLM 核心组成部分的研究，可能推动新一代规模更大的模型的发展。

**更高分辨率的多模态学习**

最近，大型视觉语言模型在其更大的模型中使用预训练的图像编码器，其中一些使用监督分类进行预训练（如PaLI，PaLI-X，Flamingo，PaLM-E），一些使用预训练的CLIP编码器（如BLIPv2，CrossTVR，ChatBridge，还有一些使用自定义多模态预训练（如 BEiT3，CoCa，SimVLM）。

**本次研究的训练方法包括三个主要组成部分：在网络规模的图像文本数据上进行图像编码器的对比性预训练，改进的 PaLI 多模态训练数据混合以及以更高分辨率进行训练。**

在单模态预训练阶段，图像编码器在 Web 上的图像文本配对上采用 SigLIP 训练协议进行对比预训练。研究人员采用了一种基于模型的过滤方法，保留了大约 40% 的配对。图像编码器在 224×224 的分辨率下进行训练。文本编码器-解码器是一个 3B UL2 模型，按照混合去噪程序进行训练。

在多模态训练阶段，研究人员将图像编码器与文本编码器-解码器结合在一起，形成了 PaLI 模型。这个模型针对多模态任务进行训练，保持图像编码器的冻结状态，使用原生分辨率（224×224）。

![](https://mmbiz.qpic.cn/mmbiz_png/5qv5QsBmI9AhXfRoD2SngubkAnQXweIO8PITwFba1URQSppibIWQ7rGMT6ic2m5tYJ6V6Sonfqgjrb1oc1MY6nMQ/640?wx_fmt=png)

主要的数据混合来自 WebLI 数据集，经过筛选和使用特定的训练目标。其他元素包括多语言字幕、OCR 处理、跨语言 VQA 和 VQG、物体感知 VQA 以及物体检测。虽然没有包括来自视频的任务或数据，但由于强大的图像编码器，PaLI-3 在这些基准上仍然具有竞争力。此外，通过向 WebLI 添加了包含稠密文本和网络图像（如海报或文档）的 PDF 文档，以及支持 100 多种语言的文本，文档和图像理解能力得到了进一步的提高。

在提高分辨率阶段，研究通过对整个模型进行微调（解冻图像编码器）并使用逐渐增加分辨率的短期课程来提高 PaLI-3 的分辨率，保持在 812×812 和 1064×1064 分辨率处的检查点。数据混合主要集中在涉及视觉定位文本和物体检测的部分。

**提升图像理解与文本定位任务**

首先，研究人员在 PaLI 框架内进行了对不同的 ViT 模型的有控制的比较。结果发现，**虽然 SigLIP 模型的少样本线性分类性能较差，但当在 PaLI-3 中使用时，SigLIP 模型在"简单"任务（如字幕和问答）上提供了适度的性能提升，并在更"复杂"的场景文本和空间理解任务（如 TextVQA 和 RefCOCO 变体）上提供了大幅提升。**

![](https://mmbiz.qpic.cn/mmbiz_png/5qv5QsBmI9AhXfRoD2SngubkAnQXweIOMeH2q1qPgp1QvkJElCHGqlicSIrBJnkhMFiaGR4drG1T5WZ9PPl2hLcA/640?wx_fmt=png)

随后，研究又在视觉定位文本理解任务中评估了 PaLI-3，这些数据集中的图像涉及自然图像、插图、文档和用户界面等各种领域。**PaLI-3 在绝大多数字幕和 VQA 基准上，无论是否有外部 OCR 输入，都取得了最先进的性能。**唯一的例外是 AI2D 和 ChartQA，它们不仅需要理解，还需要对图表进行强大的推理能力。对于这两个基准，PaLI-3 稍微落后于 PaLI-X。

另外，研究人员还扩展了 PaLI-3 的功能，使其能够通过语言类似的输出来预测分割遮罩。实验结果表明，对于这种类型的定位任务，对比预训练要比分类预训练更为有效。**完整的 PaLI-3 模型能够在指代表达分割方面稍微优于最先进的方法。**

在自然图像理解部分，研究对 PaLI-3 在通用视觉语言理解任务上进行了评估，包括 COCO 字幕和 VQAv2，**尽管与最近的 SOTA 模型相比，PaLI-3 的规模要小得多，但在这些基准上表现非常出色。**



![](https://mmbiz.qpic.cn/mmbiz_png/5qv5QsBmI9AhXfRoD2SngubkAnQXweIOFXD4rEgpmVl8osT1dQBtl1ESsiaB3pGGMrLQN3qZO7TwEmPBV0mZ2icQ/640?wx_fmt=png)

在视频字幕和问答部分，研究人员在 4 个视频字幕基准上对 PaLI-3 模型进行了微调和评估：MSR-VTT、VATEX、ActivityNet Captions 和 Spoken Moments in Time。然后，对 3 个视频问题解答基准进行了同样的测试：NExT-QA、MSR-VTT-QA 和 ActivityNet-QA。**尽管没有使用视频数据进行预训练，PaLI-3 仍然以较小的模型规模取得了出色的视频质量保证结果。**

总而言之，在本研究中，研究人员深入研究了 VLM 中图像编码器的预训练，特别是 PaLI 类型的模型。研究首次明确比较了分类预训练和图像文本（对比性）预训练这两种方法，发现后者可以带来更好和更高效的 VLM，特别是在定位和文本理解任务方面。

另外，研究人员在论文中指出：“这只是 VLM 的一个小方面，我们希望这项研究和其结果能够激励对 VLM 训练的众多其他方面进行深入探讨。”

论文链接：

https://arxiv.org/abs/2310.09199





# 7，推理

# 大模型推理框架概述

AINLP  *2023-10-08 19:16*  *发表于江苏*

从 ChatGPT 面世以来，引领了大模型时代的变革，除了大模型遍地开花以外，承载大模型进行推理的框架也是层出不穷，大有百家争鸣的态势。本文主要针对业界知名度较高的一些大模型推理框架进行相应的概述。

## vLLM

- GitHub: https://github.com/vllm-project/vllm

### 简介

vLLM是一个开源的大模型推理加速框架，通过PagedAttention高效地管理attention中缓存的张量，实现了比HuggingFace Transformers高14-24倍的吞吐量。

PagedAttention 是 vLLM 的核心技术，它解决了LLM服务中内存的瓶颈问题。传统的注意力算法在自回归解码过程中，需要将所有输入Token的注意力键和值张量存储在GPU内存中，以生成下一个Token。这些缓存的键和值张量通常被称为KV缓存。

### 主要特性

- 通过PagedAttention对 KV Cache 的有效管理

- 传入请求的continus batching，而不是static batching

- 支持张量并行推理

- 支持流式输出

- 兼容 OpenAI 的接口服务

- 与 HuggingFace 模型无缝集成

### 与其他框架（HF、TGI）的性能对比

vLLM 的吞吐量比 HF 高 14 - 24 倍，比 TGI 高 2.2 - 2.5 倍。

![](https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGtytcgekXZcet2F3a6IHFViaFVOD5lvfjld3zmsUcUyGY1nvbjCdtjich888dAOe7ESC5zT06iaarwQ/640?wx_fmt=png)

image.png

### 存在的问题

- 同样的模型、参数和prompt条件下，vLLM推理和Huggingface推理结果不一致。具体请参考：https://zhuanlan.zhihu.com/p/658780653

### 业界案例

vLLM 已经被用于 Chatbot Arena 和 Vicuna 大模型的服务后端。

## HuggingFace TGI

- GitHub: https://github.com/huggingface/text-generation-inference

## 为什么现在大家都在用 MQA 和 GQA？

安迪的写作间  安迪的写作间  *2023-07-30 20:33*  *发表于北京*

https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q

> > 好几周前读完 GQA 论文就想写的，但一直拖着。直到最近 LLAMA2，还有 NV 小伙伴问到我这一点，我解释了下，他让我顺便写下来写成一篇文章吧。
> > 
> > 我想着推导一下 MQA 就好了，但发现这个话题并没那么简单，这是由一系列限制导致的优化，涉及到模型结构甚至涉及到硬件，我个人最开始也没看懂 MQA 操作。于是一写就各种瞎扯了。
> 
> MQA，全称 Multi Query Attention, 而 GQA 则是前段时间 Google 提出的 MQA 变种，全称 Group-Query Attention.
> 
> MQA 提出时间挺早的，是  **Noam Shazeer**  这位谷歌老炮 19 年提出的。而 Noam 也是 Transformer 结构提出者之一，现在也就理所当然地早就不在 Google，是 Character.ai 的合伙人。
> 
> ![](https://mmbiz.qpic.cn/mmbiz_png/Gyc8xouDEr2bwlca85RkIZmbU9LvcIxvMdx1kmE5icfXvOQEWgTwibRdNd8byGGVfxyiaZohtpgE2vDhoYkOdHVOg/640?wx_fmt=png)
> 
> ![](https://mmbiz.qpic.cn/mmbiz_png/Gyc8xouDEr2bwlca85RkIZmbU9LvcIxv0yYC8XL2FFcN03rDfMnCLJFJ2TWvBgrDic75dj9KBTXhakJEDHtrwRg/640?wx_fmt=png)
> 
> 这位老哥对 Transformer 提出过好几种结构改进，比如 Talking-Heads、GLU 激活、还有这里谈的 MQA，他这些论文都简单粗暴，上方法看效果，不玩虚的，主打一个好用。
> 
> MQA 提出时，并没获得太大关注，包括我也是翻 Talking-Heads 才知道这篇论文，当时看了下也没觉得哪里好，就忘了。而且作者可能也没太当一回事，论文从头到脚都能看到两个字，随意。
> 
> 最近才越来越多被提到，包括 Falcon，Star Coder，还有最近的 Llama 2 都有用到。
> 
> 心里疑惑，为什么之前就没有被关注呢？
> 
> ### GPT 的阿克琉斯之踵
> 
> GPT，也就是 Transformer Decoder 结构做文本生成时有一个致命问题。
> 
> 先来看看 Encoder 推理是怎么做的，每个 timestep 都能看到所有 timestep ，推理时所有 timestep 一层层向后计算，一把过。
> 
> 于是内存相关开销就是 , 而计算相关开销就是 ，其中 N 为序列长度。
> 
> ![](https://mmbiz.qpic.cn/mmbiz_png/Gyc8xouDEr2bwlca85RkIZmbU9LvcIxvDpGheLdbru6U3jHKAwR9Q1KTUo6A4tQBWIpVHTewTz7fcgfNapFJZw/640?wx_fmt=png)
> 
> 而 Decoder 推理时，最大不同在于**自回归结构**，可以看到图中每个 timestep 的输出都是下一 timestep 的输入，所以无法像 Encoder 一样一次过，每次都要 attend 之前的所有 timestep.
> 
> 同样计算一下开销，计算开销是 也就是 ，而内存开销则是 O().
> 
> 大家用 ChatGPT 接口也会有类似感觉，Context 部分成本很低，也很快，因为它做的类似于 Encoder 的并行。主要成本在生成那块，速度较慢，但也已经是优化过后的了。
> 
> 下面就来讲讲优化方法。
> 
> ### KV Cache
> 
> Decoder 每次前向，当前 timestep 计算 Attention 要用到的部分，如之前 timestep 的 KV （Key 和 Value）值都计算过的，只是之前每次前向完后给计算结果都丢掉，只保留最后输出。
> 
> 于是一个很自然的想法就是  **Cache**。这很像斐波那契递归函数，naive 版本，也会出现不断重复计算问题，加个 cache 瞬间提速。
> 
> 每次前向完，给  **KV 都保留下来，用于之后计算**。
> 
> 代码表示如下（关于 Self-Attention 不清楚可看我这篇  [《Transformer 三部曲：RNN 的继承者》](https://mp.weixin.qq.com/s?__biz=MzUxMjE1NzA2MA==&mid=2247483922&idx=1&sn=28ff93a669b5ae2f722c2ce8b6e9b719&scene=21#wechat_redirect)）：
> 
> `#q、k、v 当前 timestep 的 query，key，value  
> 
> # K_prev,V_prev 之前所有 timestep 的 key 和 value
> 
> for _ in range(time_step):  
>     ...  
>     K = torch.cat([K_prev, k], dim=-2) #[b, h, n, d]  
>     V = torch.cat([V_prev, v], dim=-2) #[b, h, n, d]  
> 
>     logits = torch.einsum("bhd,bhnd->bhn", q, K)  
>     weights = torch.softmax(logits/math.sqrt(d), dim=-1)  
>     outs = torch.einsum("bhn,bhnd->bhd", weights, V)  
>     ...  
>     
>     K_prev, V_prev = K, V  
> 
> `
> 
> 于是 Decoder 就被优化成，计算开销变成了 O()，存储复杂度则是 O()，只给 K 和 V 不断保存在缓存中就行。问题解决了！
> 
> 但残酷现实会立马跳出来给你一棒子，上面假设 K 和 V 能直接存在缓存中，模型规模小还好，一旦模型规模很大长度很长时，**KV 根本就存不进缓存**。
> 
> 比如 Llama 7B 模型，hidden size 是 4096，那么每个 timestep 需缓存参数量为 4096*2*32=262144，假设半精度保存就是 512KB，1024 长度那就要 512MB. 而现在英伟达最好的卡 H100 的 SRAM 缓存大概是 50MB，而 A100 则是 40MB. 而 7B 模型都这样，175B 模型就更不用说了。
> 
> 那为什么我们不直接做大 SRAM 内存呢，不就直接解决问题了吗，但是这样又会产生一个新问题  **SRAM 太贵了**，所以这条路现在是不太行的。
> 
> 于是退一步，放不进缓存可以放 DRAM 上去，而 DRAM 内存也就是我们常说的 GPU 显存。
> 
> 但 DRAM 读取到计算芯片和 SRAM 到计算芯片的速度，差了一个量级的，这会让计算芯片一直在等待。
> 
> 现在我们遇到了当今芯片领域，冯诺依曼架构下最大的一个问题，也就是：**Memory Wall（内存墙）**。
> 
> ### 冯诺依曼架构和 Memory Wall
> 
> ![](https://mmbiz.qpic.cn/mmbiz_png/Gyc8xouDEr2bwlca85RkIZmbU9LvcIxvf9ZZ2sMn3Up9ml0wR8G2e1Kgicu4ulqHuVJjOS03hmw7hrFysrpDlWQ/640?wx_fmt=png)
> 
> 冯诺依曼架构熟悉有计算机相关基础的，应该都稔熟于胸。输入，输出，计算单元，加上存储单元。
> 
> 现在随着摩尔定律的见顶，虽然计算和内存的发展速度在变缓，但这并不是最大的问题，最大的问题是**存储单元与计算单元间的交互**。
> 
> 冯诺依曼架构需要先**从内存中调取数据**，**送入计算单元进行处理**，但现在计算单元的速度是显著提升的，而从内存中读取数据的速度却没跟上，所以计算和内存这里就形成了一个瓶颈。因为短板效应，内存读取速度限制了整体速度。计算单元能很快将数据处理完，但新数据却还没到，于是就只能等待，造成利用率不高。这就是**内存墙**。
> 
> 因为内存墙问题，现在 GPU，一张 A100 卡计算单元的利用率到四五十就不错了，用上各种技巧优化到 60% 已经很高了。而对于 H100 卡问题会更严重，因为它的计算速度相对 A100 提高了 6 倍，而内存读取带宽只增加了 1.6 倍，所以也要大量优化来提高利用率。
> 
> 内存墙怎么越过呢？
> 
> 硬件层面上，比如现在已在使用的  **HBM（高速带宽内存）**提高读取速度，或者更彻底些，抛弃冯诺依曼架构，改变计算单元从内存读数据的方式，不再以计算单元为中心，而以存储为中心，做成计算和存储一体的“**存内计算**”。
> 
> 软件层面上的话，最近的很多优化，比如  **Flash Attention**，**Paged Attention**  都可以算。Flash Attention 就是减少了计算 Softmax 时从 DRAM 内存读取数据次数，从而提高了效率。
> 
> 同样，MQA 也是一个软件层面上翻墙的一个方法。
> 
> ### MHA 到 MQA 到 GQA
> 
> MQA 的方法很简单，难的是看到这样的方法后，能立刻想到它为什么好。
> 
> 一起看看 MQA 和 GQA 是怎么来的。
> 
> ![](https://mmbiz.qpic.cn/mmbiz_png/Gyc8xouDEr2bwlca85RkIZmbU9LvcIxvxr2rISYdR26EcGtwntrsXsKXvn1ia5cegII4muYQ2vrZTvpJnrkW4aw/640?wx_fmt=png)
> 
> 首先是原始的  **MHA(Multi-Head Attention)**，QKV 三部分有相同数量的头，且一一对应。每次做 Attention，head1 的 QKV 就做好自己运算就可以，输出时各个头加起来就行。
> 
> 而 MQA 则是，让  **Q 仍然保持原来的头数**，但  **K 和 V 只有一个头**，相当于所有的 Q 头共享一组 K 和 V 头，所以叫做 Multi-Query 了。实现改变了会不会影响效果呢？确实会影响但相对它能带来的收益，性能的些微降低是可以接受的。
> 
> 能带来多大的收益呢，实验发现一般能提高 30%-40% 的吞吐。
> 
> 收益主要就是由降低了 KV cache 带来的。实际上 MQA 运算量和 MHA 是差不多的，可理解为**读取一组 KV 头**之后，**给所有 Q 头用**，但因为之前提到的内存和计算的不对称，所以是有利的。
> 
> 而 GQA 呢，是 MHA 和 MQA 的折衷方案，既不想损失性能太多，又想获得 MQA 带来的推理加速好处。具体思想是，不是所有 Q 头共享一组 KV，而是**分组一定头数 Q 共享一组 KV**，比如上面图片就是两组 Q 共享一组 KV。
> 
> LLAMA2 中给出了效果对比，可以看到相比起 MQA，GQA的指标看起来还是要好些的。
> 
> ![](https://mmbiz.qpic.cn/mmbiz_png/Gyc8xouDEr2bwlca85RkIZmbU9LvcIxvkvPuDNO1zfd034gwXwpuibyE3V7BbWfXnklGQsbgM50UlukXibGMFAGw/640?wx_fmt=png)
> 
> 同时在推理上的加速还和 MQA 类似：
> 
> ![](https://mmbiz.qpic.cn/mmbiz_png/Gyc8xouDEr2bwlca85RkIZmbU9LvcIxvqUGHqw0hmQQkccxOER183d2hh5RS98aMeAgE8ibXicjmBTdZm1jjAj4w/640?wx_fmt=png)
> 
> MQA 和 GQA 形式在推理加速方面，主要是通过两方面来完成：
> 
> 1. **降低了从内存中读取的数据量**，所以也就减少了计算单元等待时间，提高了计算利用率；
> 
> 2. KV cache 变小了 head_num 倍，也就是显存中需要保存的 tensor 变小了，**空出来空间就可以加大 batch size**，从而又能提高利用率。
> 
> 如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA 论文里面一样，用已有的开源模型，挑一些头取个 mean 用来初始化 MQA 或 GQA 继续训练一段时间。
> 
> 下面是 MQA 推导过程，不感兴趣同学可跳过，感兴趣同学可推一下，理解更透彻。
> 
> #### MQA 的推导
> 
> 正如在 memory wall 中提到的，现在内存读取相对计算速度太慢导致拖后腿。
> 
> 那么定义一个变量，， M 是 Memory 表示内存开销，而 A 是 Arithmetic 表示计算开销。如果这个值大于1的话，就会出现很明显的 Memory Wall，而当这个值小于1很多时，表示拿到数据后马上能开动马力计算，内存墙问题就不存在了。因为估算还有各种没考虑因素问题，所以即使等于 1 也不代表就能打满计算单元。
> 
> 那么先来看看 MHA 下推理时每一个 timestep 这个值的大小，主要参考 MQA 原论文的简化：
> 
> `#三个投影矩阵分别为 P_q, P_k, P_v; 维度为 h(头数), a（隐层大小,等于hd）, d（每个头大小）  
> 
> #当前 timestep 输入为 x，维度为 b(batch大小), a  
> 
> #K_prev, V_prev 为 KV cache的矩阵，维度为 b, h, m(之前的timestep数)，d; m+1=n  
> q = torch.einsum('ba,had->bhd', x, P_q) #M:had+ba, A:ba^2  
> k = torch.einsum('ba,had->bhd', x, P_k) #M:had+ba, A:ba^2  
> v = torch.einsum('ba,had->bhd', x, P_v) #M:had+ba, A:ba^2  
> K = torch.cat([K_prev, k.unsqueeze(2)], dim=-2) #M:bhnd+bhd, A:0  
> V = torch.cat([V_prev, v.unsqueeze(2)], dim=-2) #M:bhnd+bhd, A:0  
> logits = torch.einsum("bhd,bhnd->bhn", q, K)#M:bhnd+bhd, A:bhnd  
> weights = torch.softmax(logits/math.sqrt(d), dim=-1)#M:bhn  
> outs = torch.einsum("bhn,bhnd->bhd", weights, V)#M:bhn+bhnd, A:bhnd  
> `
> 
> 所以对于 M 来说是
> 
> 对于 A 来说
> 
> 假设隐层大小和 timestep 数接近，, 那么 A 就是 , 因此
> 
> 可以看到要想让这个比例小，可以增大b，也就是增大 batch size，现在推理优化就会将用户的请求收集成 batch 推理，提高利用率。同时前面提到，MQA 可以降低显存使用扩大 batch size，所以能提高一定利用率。
> 
> 根据假设 ，这个比例会接近 1，会导致一定 Memory Wall，如果 n 很长的话问题就更明显。
> 
> 而 MQA 的情况下
> 
> `#投影矩阵 P_k, P_v 维度变为 a（隐层大小,等于hd）, d（每个头大小）  
> 
> #K_prev, V_prev 为 KV cache的矩阵，维度为 b, m(之前的timestep数)，d; m+1=n  
> q = torch.einsum('ba,had->bhd', x, P_q) #M:had+ba, A:ba^2  
> k = torch.einsum('ba,ad->bd', x, P_k) #M:ad+ba, A:bad  
> v = torch.einsum('ba,ad->bd', x, P_v) #M:ad+ba, A:bad  
> K = torch.cat([K_prev, k.unsqueeze(1)], dim=-2) #M:bnd+bd  
> V = torch.cat([V_prev, v.unsqueeze(1)], dim=-2) #M:bnd+bd  
> logits = torch.einsum("bhd,bnd->bhn", q, K)#M:bhd+bnd, A:bhnd  
> weights = torch.softmax(logits/math.sqrt(d), dim=-1)#M:bhn  
> outs = torch.einsum("bhn,bnd->bhd", weights, V)#M:bhn+bnd, A:bhnd  
> `
> 
> 会发现 A 整体来说没有变，如之前说的只是共享了 KV, 计算量还是一样的 ，M 变化比较大
> 
> 于是系数为
> 
> 其中后面两项，d 一般比 h 要大，所以可以主要考虑 项。可看到之前占大头的 在分母加了个系数 h，这样就能降低 从而提高效率。
> 
> 感兴趣的话，可自己推导一下 GQA 的情况，其中 的分母中会加入一个数 , 其中 为 group 数，如果 为 1 的情况那就和 MQA 一样了，这块开销主要就有 g 来调整了。
> 
> ### 再见美好旧时光
> 
> 看到这，大概也能明白为什么要用 MQA 了，以及为什么 MQA 最近才突然火起来。
> 
> 主要就是因为**大规模 GPT 式生成模型的落地需求导致的**。
> 
> 而在以前根本不需要关心这些，LSTM 只用维护一个状态，不存在要保留 Cache 什么。
> 
> 到了 Transformer 提出后，虽然最早 Transformer 提出时是用在 Seq2Seq 任务上，也就是 Encoder 和 Decoder 都用，但可能模型量级不大，也没有太多落地需求，所以没引起太大关注。之后火了两年的 BERT 又是 Encoder 结构，直接前向一把过。
> 
> 也只有到最近 GPT 大模型得到广泛应用时，才发现推理的这个瓶颈，于是大家翻出几年前的 trick，应用起来，发现非常好用。
> 
> 同样原因，GPT 推理加速这块最近引起很多关注，大家都在想各种方法来提高推理效率。Huggingface 这两天也给 text-generation-inference 库的 license 给改了，应该也是想用这个挣点钱。
> 
> ![](http://mmbiz.qpic.cn/mmbiz_png/Gyc8xouDEr1Fuico5bDtvNMr65FkSQApRkHF7J0cYThkDejMclz0y1sBcxGW3ibLVBqY8AyK8sKmkByic57l406tA/300?wx_fmt=png&wxfrom=19)
> 
> **安迪的写作间**
> 
> LLM 算法工程师，热爱生活热爱有趣，反卷小斗士
> 
> 110篇原创内容
> 
> 公众号
> 
> ### Reference
> 
> 1. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
> 
> 2. Multi-Query Attention is All You Need
> 
> 3. Fast Transformer Decoding: One Write-Head is All You Need
> 
> 4. 芯片简史
> 
> 5. How Nvidia’s CUDA Monopoly In Machine Learning Is Breaking - OpenAI Triton And PyTorch 2.0
